import os
import torch
import numpy as np
from .symquant8bit import SymQuant8bit
from model import BitGateNet, Conv2d_Q, Linear_Q

# === Load model ===
model = BitGateNet(num_classes=8, quantscale=1.0)
model.load_state_dict(torch.load("checkpoints/b_m_0.3924.pth", map_location="cpu"))

# === Extract modules with weights or biases ===
def extract_named_modules_with_params(model):
    return [(name, mod) for name, mod in model.named_modules()
            if any(k in mod.state_dict() for k in ("weight", "bias", "param"))]

# === Quantize and collect metadata ===
quantizer = SymQuant8bit()
if __name__=="__main__":
    
    extracted_data = []

    for name, module in extract_named_modules_with_params(model):
        weight = module.state_dict().get("weight", None)
        print(weight)
        # q_weight, _ = quantizer.quantize(weight)
        weight_shape = list(weight.shape) if weight is not None else None
        print(weight_shape)

        # bias = module.state_dict().get("bias", None)
        # bias_shape = list(bias.shape) if bias is not None else None

        # params = module.state_dict().get("param", None)
        # params_shape = list(params.shape) if params is not None else None

        # extracted_data.append((name, weight_shape, bias_shape, params_shape))
    #print(extracted_data[0])

# # === Macro generation ===
# macro_lines = []
# macro_lines.append("// Auto-generated by export_m.py")
# macro_lines.append("#ifndef WEIGHTS_H")
# macro_lines.append("#define WEIGHTS_H")

# for name, w_shape, b_shape in extracted_data:
#     parts = name.split(".")
#     macro_name = "_".join(part.upper() for part in parts)

#     if len(w_shape) == 4:
#         macro_lines.append(f"#define {macro_name}_OUT_CH {w_shape[0]}")
#         macro_lines.append(f"#define {macro_name}_IN_CH {w_shape[1]}")
#         macro_lines.append(f"#define {macro_name}_KER_H {w_shape[2]}")
#         macro_lines.append(f"#define {macro_name}_KER_W {w_shape[3]}")
#     elif len(w_shape) == 2:
#         macro_lines.append(f"#define {macro_name}_IN_CH {w_shape[0]}")
#         macro_lines.append(f"#define {macro_name}_OUT_CH {w_shape[1]}")
#     elif len(w_shape) == 1:
#         macro_lines.append(f"#define {macro_name}_PARAM_SIZE {w_shape[0]}")

#     if b_shape is not None:
#         macro_lines.append(f"#define {macro_name}_HAS_BIAS 1")
#         macro_lines.append(f"#define {macro_name}_BIAS_SIZE {b_shape[0]}")

# macro_lines.append("#endif")

# # === Write to header file ===
# header_path = "tiny_c/weights.h"

# if not os.path.exists(header_path):
#     with open(header_path, "w") as f:
#         f.write("\n".join(macro_lines))
#     print(f"[✓] Header written to: {header_path}")
# else:
#     print("[!] File already exists. Override skipped.")


# c_lines = []

# for tup in extracted_data:
#     layer_name = tup[0]
#     weight_str = tup[2]
#     bias_str = tup[3]

#     # Format name like CONV1, BLOCK1_LAYER1
#     parts = layer_name.split(".")
#     var_name = parts[0].upper()
#     if len(parts) > 1:
#         var_name += "_" + parts[1].upper()

#     # Write weights array
#     c_lines.append(f"const int8_t {var_name}_weights[] = {{ {weight_str} }};")

#     # Optional: Write bias array if exists
#     if bias_str is not None:
#         c_lines.append(f"const float {var_name}_bias[] = {{ {bias_str} }};")

# # Add file guards and includes
# header = "#include <stdint.h>\n"
# c_text = header + "\n".join(c_lines) + "\n"

# # Save to file
# out_c = "tiny_c/weights.c"
# with open(out_c, "w") as f:
#     f.write(c_text)

# print(f"[✓] weights.c written to: {out_c}")
